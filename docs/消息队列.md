# 消息队列
## 消息队列有什么用？

通常来说，使用消息队列主要能为我们的系统带来下面三点好处：

1. 异步处理：请求中包含的耗时操作，通过消息队列实现异步处理，将对应的消息发送到消息队列之后就立即返回结果，减少响应时间，提高用户体验。随后，系统再对消息进行消费。
2. 削峰/限流：先将短时间高并发产生的事务消息存储在消息队列中，然后后端服务再慢慢根据自己的能力去消费这些消息，这样就避免直接把后端服务打垮掉。 
3. 降低系统耦合性：使用消息队列还可以降低系统耦合性。如果模块之间不存在直接调用，那么新增模块或者修改模块就对其他模块影响较小，这样系统的可扩展性无疑更好一些。

除了这三点之外，消息队列还有其他的一些应用场景，例如实现分布式事务、顺序保证和数据流处理。

-----

## JMS

### JMS 是什么？

JMS（JAVA Message Service,java 消息服务）是 Java 的消息服务，JMS 的客户端之间可以通过 JMS 服务进行异步的消息传输。JMS（JAVA Message Service，Java 消息服务）API 是一个消息服务的标准或者说是规范，允许应用程序组件基于 JavaEE 平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。

JMS 定义了五种不同的消息正文格式以及调用的消息类型，允许你发送并接收以一些不同形式的数据：

* StreamMessage：Java 原始值的数据流
* MapMessage：一套名称-值对
* TextMessage：一个字符串对象
* ObjectMessage：一个序列化的 Java 对象
* BytesMessage：一个字节的数据流

### JMS 两种消息模型

* 点到点（P2P）模型：使用队列（Queue）作为消息通信载体；满足生产者与消费者模式，一条消息只能被一个消费者使用，未被消费的消息在队列中保留直到被消费或超时。

![img.png](public/消息队列1.png)
* 发布/订阅（Pub/Sub）模型：使用主题（Topic）作为消息通信载体，类似于广播模式；发布者发布一条消息，该消息通过主题传递给所有的订阅者。在一条消息广播之后才订阅的用户则是收不到该条消息的。

![img.png](public/消息队列2.png)

| **对比维度**       | **点对点（P2P）模式**                          | **发布订阅（Pub/Sub）模式**                    |
|--------------------|------------------------------------------------|------------------------------------------------|
| **通信方式**       | 一对一：消息从生产者发送到特定队列，仅被一个消费者接收。 | 一对多：消息从生产者发布到主题，所有订阅该主题的消费者均可接收。 |
| **消息生命周期**   | 消费者接收并确认后，消息从队列中删除（默认）。   | 消息发布后，与订阅者的接收状态无关，通常按保留策略自动删除。 |
| **参与者关系**     | 生产者和消费者需明确知道队列名称，耦合度较高。   | 生产者只关注主题，消费者只关注订阅，两者完全解耦，无需知晓对方存在。 |
| **典型中间件实现** | RabbitMQ（队列模式）、ActiveMQ（Queue）         | Kafka（Topic）、RabbitMQ（Exchange+Topic）、Redis Pub/Sub |
| **消息重复消费风险** | 低（单消费者处理）                             | 高（多消费者可能各自处理，需通过幂等性控制）       |
|**例子**|如客户端向服务端发送请求，服务端处理后返回结果，避免多个服务同时响应同一请求。|数据同步：多系统需要同步同一批数据，如电商商品价格变更后，搜索系统、推荐系统、缓存系统需同时更新。|

----

## [Kafka](https://github.com/Snailclimb/JavaGuide/blob/main/docs/high-performance/message-queue/kafka-questions-01.md)
### Kafka 核心概念
1. Producer、Consumer、Broker、Topic、Partition
   * Broker：Kafka 服务器实例，存储消息日志。集群由多个 Broker 组成，每个 Broker 可管理多个 Topic。
   * Topic：消息分类的逻辑概念，数据发布的目的地。物理上分为多个 Partition。 
   * Partition：Topic 的物理分片，消息按顺序存储在分区中，每个分区是一个有序的日志文件。 
   * Replica：Partition 的副本，保障高可用。一个 Partition 有多个 Replica，其中一个为 Leader，其余为 Follower。 
   * Producer：消息生产者，将消息发布到指定 Topic。 
   * Consumer：消息消费者，从 Topic 拉取消息。Consumer 属于 Consumer Group，Group 内的 Consumer 共同消费 Topic 的分区。 
   * ZooKeeper：管理 Kafka 集群元数据（如 Broker 注册、Partition 状态），新版本（Kafka 3.0+）逐步移除对 ZooKeeper 的依赖。
2. 流式处理平台具有三个关键功能：
   * 消息队列：发布和订阅消息流，这个功能类似于消息队列，这也是 Kafka 也被归类为消息队列的原因。
   * 容错的持久方式存储记录消息流：Kafka 会把消息持久化到磁盘，有效避免了消息丢失的风险。
   * 流式处理平台： 在消息发布的时候进行处理，Kafka 提供了一个完整的流式处理类库。

### Kafka 如何保证消息不丢失？
丢失场景：producer发送给broker/broker储存/consumer接收消息
#### 生产者发送消息时丢失
* 设置异步发送（为其添加回调函数）
```java
    ListenableFuture<SendResult<String, Object>> future = kafkaTemplate.send(topic, o);
    future.addCallback(result -> logger.info("生产者成功发送消息到topic:{} partition:{}的消息", result.getRecordMetadata().topic(), result.getRecordMetadata().partition()),
        ex -> logger.error("生产者发送消失败，原因：{}", ex.getMessage()));
```
* 设置重试次数
```java 
prop.put(ProducerConfig.RETRIES_CONFIG, 10);
```

#### 消息在broker中储存时丢失
* 发送确认机制: 设置 acks = **all**:
```text
    acks 是 Kafka 生产者(Producer) 很重要的一个参数。
    acks = 0: 写入消息不等待任何回复
    acks = 1(默认值): 只要 leader 节点接收之后就算被成功发送;
    acks = all： 所有副本全部收到消息，生产者才会接收到来自服务器的响应. 这种模式是最高级别的，也是最安全的，可以确保不止一个 Broker 接收到了消息. 该模式的延迟会很高.
```
* 设置 replication.factor >= 3: 为了保证 leader 有 follower 同步消息，一般会为 topic 设置 replication.factor >= 3。这样就可以保证每个 分区(partition) 至少有 3 个副本。虽然造成了数据冗余，但是带来了数据的安全性。
* 设置 min.insync.replicas > 1: ISR(in-sync replica)需要同步复制保存的follower. 这样配置代表消息至少要被写入到 2 个副本才算是被成功发送。min.insync.replicas 的默认值为 1 ，在实际生产中应尽量避免默认值 1。

#### 消费者消费消息时丢失 
消费者默认每隔5s提交一次已经消费的offset
* 禁用自动偏移量提交```enable.auto.commit=false```, 改为手动提交```consumer.conmmitAsync(); consumer.conmmitSync();```

### Kafka 如何保证消息的消费顺序？
* 1 个 Topic 只对应一个 Partition。
* （推荐）利用分区内有序，发送消息的时候指定 Key/Partition：Kafka 中发送消息的时候，可以指定 topic, partition, key, data（数据） 4 个参数。如果你发送消息的时候指定了 Partition 的话，所有消息都会被发送到指定的 Partition。并且，同一个 key 的消息可以保证只发送到同一个 partition，这个我们可以根据业务采用表/对象的 id 来作为 key 。

### Kafka 如何保证消息不重复消费？
#### kafka 出现消息重复消费的原因： 
1. 服务端侧已经消费的数据没有成功提交 offset（根本原因）。 
2. Kafka 侧由于服务端处理业务时间长或者网络链接等等原因让 Kafka 认为服务假死，触发了分区 rebalance。
#### 解决方案：
1. 消费消息服务做幂等校验，比如 Redis 的 set、MySQL 的主键等天然的幂等功能。这种方法最有效。（消息消费时，因网络重试、队列重试等原因，可能出现同一条消息被多次投递。幂等性即要求：多次处理同一条消息的结果，与处理一次完全一致（比如避免重复扣款、重复创建订单）） 
2. 禁用自动偏移量提交```enable.auto.commit=false```，开发者在代码中手动提交 offset。那么这里会有个问题：什么时候提交 offset 合适？
   * 处理完消息再提交：依旧有消息重复消费的风险，和自动提交一样;
   * 拉取到消息即提交：会有消息丢失的风险。允许消息延时的场景，一般会采用这种方式。然后，通过定时任务在业务不繁忙（比如凌晨）的时候做数据兜底。

### Kafka 数据清理机制 
* 超过指定时间则被清理（默认7天）
* 根据topic储存的数据大小，超过则删除最老的（默认关闭）

### 死信队列（Dead Letter Queue, DLQ）

死信队列是消息中间件（如 RabbitMQ、Kafka、RocketMQ）提供的一种特殊机制，用于存储无法被正常消费的消息。过后再人工进行处理。

当消息出现以下情况时，若直接丢弃会导致数据丢失，若一直重试会消耗资源并阻塞队列：
* 多次消费失败：例如消息处理逻辑有 bug，重试 N 次仍失败。
* 消息超时：消费者处理时间超过预设阈值（如 30 分钟）。
* 队列达到最大长度：队列已满，新消息无法入队。
* 消息被显式拒绝：业务逻辑中主动拒绝某类消息（如格式错误的消息）。

死信队列的核心作用是隔离异常消息，保证主业务流程不受影响，同时为后续分析和修复提供数据基础

### 有100万条数据堆积在队列里怎么办
* 增加消费者
* 在消费者内开启线程池
* 扩大队列容积